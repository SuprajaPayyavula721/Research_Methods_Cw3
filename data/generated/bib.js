define({ entries : {
    "BonetSola2023Analysis": {
        "abstract": "The Sons al Balc\u00f3 project uses citizen science to study changes in Catalonia's soundscape during and after the COVID-19 lockdown. They collected videos from 2020 and 2021 and employed a convolutional neural network (CNN) to automatically detect and classify sound events. Results show the CNN performed well, achieving over 50% accuracy in identifying prevalent noise sources. However, some categories were detected better than others, influenced by factors like event prevalence and foreground-to-background ratio. This approach highlights the potential of combining citizen science and machine learning to understand and manage urban soundscapes.",
        "author": "Bonet-Sol\u00e0 D, Vida\u00f1a-Vila E, Alsina-Pag\u00e8s R M",
        "doi": "10.3390/ijerph20043683",
        "journal": "International Journal of Environmental Research and Public Health",
        "keywords": "citizen science, acoustic event detection, noise annoyance, convolutional neural networks",
        "number": "4",
        "title": "Analysis and Acoustic Event Classification of Environmental Data Collected in a Citizen Science Project",
        "type": "article",
        "url": "https://doi.org/10.3390%2Fijerph20043683",
        "volume": "20",
        "year": "2023"
    },
    "ClinkKierAhmadEtAl2023Workflow": {
        "abstract": "This paper presents a workflow for the automated detection and classification of female gibbon calls from long-term acoustic recordings. The study utilizes advanced signal processing techniques and machine learning algorithms to identify and categorize gibbon vocalizations. Results demonstrate the effectiveness of the workflow, achieving high accuracy in detecting and classifying female gibbon calls. The developed methodology holds promise for studying gibbon behavior and ecology, as well as informing conservation efforts for these endangered species.",
        "author": "Dena J. Clink, Isabel Kier, Abdul Hamid Ahmad, Holger Klinck.",
        "doi": "10.3389/fevo.2023.1071640",
        "journal": "Frontiers in Ecology and Evolution",
        "keywords": "automated detection, classification, gibbon calls, acoustic recordings, machine learning",
        "title": "A Workflow for the Automated Detection and Classification of Female Gibbon Calls from Long-Term Acoustic Recordings",
        "type": "article",
        "url": "https://doi.org/10.3389/fevo.2023.1071640",
        "volume": "11",
        "year": "2023"
    },
    "DasGhoshPalEtAl2020UrbanSound": {
        "abstract": "Das et al Present a study on urban sound classification using a combination of Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) based on multiple features. Their approach utilizes various features to classify urban sounds effectively. The CNN-LSTM model demonstrates promising results in accurately classifying urban sound events. By leveraging multiple features and advanced neural network architectures, this work contributes to the advancement of urban sound classification techniques.",
        "author": "Joy Krishan Das, Arka Ghosh, Abhijit Kumar Pal, Amitabha Chakrabarty",
        "doi": "10.1109/ICDS50568.2020.9268723",
        "journal": "IEEE International Conference on Data Science (ICDS)",
        "keywords": "urban sound classification, convolutional neural network, long short term memory, multiple features",
        "title": "Urban Sound Classification Using Convolutional Neural Network and Long Short Term Memory Based on Multiple Features",
        "type": "article",
        "url": "https://doi.org/10.1109/ICDS50568.2020.9268723",
        "year": "2020"
    },
    "GibbEldridgeSandomEtAl2024Interpretable": {
        "abstract": "In their study, Gibb et al. introduce a novel approach towards interpretable learned representations for ecoacoustics using variational auto-encoding (VAE). They explore the application of VAEs to ecoacoustic data, aiming to uncover meaningful and interpretable patterns in acoustic recordings. Their results demonstrate the efficacy of the proposed method in capturing latent features of ecoacoustic environments while providing interpretable representations. This work contributes to advancing the understanding of ecoacoustic systems and may facilitate various ecological monitoring and conservation efforts.",
        "author": "K.A. Gibb, A. Eldridge, C.J. Sandom, I.J.A. Simpson.",
        "doi": "10.1016/j.ecoinf.2023.102449",
        "journal": "Ecological Informatics",
        "keywords": "ecoacoustics, interpretable representations, variational auto-encoding, ecological monitoring, conservation",
        "title": "Towards Interpretable Learned Representations for Ecoacoustics Using Variational Auto-encoding",
        "type": "article",
        "url": "https://doi.org/10.1016/j.ecoinf.2023.102449",
        "volume": "80",
        "year": "2024"
    },
    "LakdariEtAl2024Discrimination": {
        "abstract": "In their study, Lakdari et al. (2024) investigate discrimination tasks of individual gibbons using mel-frequency cepstral coefficients (MFCCs) and embeddings from pre-trained convolutional neural networks (CNNs) under noisy conditions. The research finds that MFCCs outperform CNN embeddings in noisy environments, demonstrating superior performance for discriminating individual gibbons. This suggests the effectiveness of MFCCs for acoustic discrimination tasks in challenging conditions.",
        "author": "Mohamed Walid Lakdari, Abdul Hamid Ahmad, Sarab Sethi, Gabriel A. Bohn, Dena J. Clink.",
        "doi": "10.1016/j.ecoinf.2023.102457",
        "journal": "Ecological Informatics",
        "keywords": "acoustic discrimination, mel-frequency cepstral coefficients, convolutional neural networks, gibbons",
        "title": "Mel-frequency Cepstral Coefficients Outperform Embeddings from Pre-trained Convolutional Neural Networks under Noisy Conditions for Discrimination Tasks of Individual Gibbons",
        "type": "article",
        "url": "https://doi.org/10.1016/j.ecoinf.2023.102457",
        "volume": "80",
        "year": "2024"
    },
    "MiaoEtAl2023ZeroShotTransfer": {
        "author": "Miao Z, Elizalde B, Deshmukh S, Kitzes J, Wang H, Dodhia R, & Lavista Ferres JM.",
        "doi": "10.21203/rs.3.rs-3180218/v1",
        "title": "Zero-Shot Transfer for Wildlife Bioacoustics Detection",
        "type": "article",
        "url": "https://doi.org/10.21203/rs.3.rs-3180218/v1",
        "year": "2023"
    },
    "NietoMora2023Systematic": {
        "abstract": "This systematic review explores the application of machine learning methods in ecoacoustics and soundscape monitoring. The study synthesizes findings from existing literature to assess the effectiveness of machine learning techniques in analyzing acoustic data for ecological and environmental purposes. By examining a range of studies, the review identifies trends, challenges, and opportunities in this interdisciplinary field. The review contributes to the understanding of how machine learning can enhance our knowledge of ecoacoustics and aid in soundscape monitoring efforts.",
        "author": "D.A. Nieto-Mora, Susana Rodr\u00edguez-Buritica, Paula Rodr\u00edguez-Mar\u00edna, J.D. Mart\u00ednez-Vargaz, Claudia Isaza-Narv\u00e1ez.",
        "doi": "10.1016/j.heliyon.2023.e20275",
        "journal": "Heliyon",
        "keywords": "machine learning, ecoacoustics, soundscape monitoring, systematic review",
        "title": "Systematic Review of Machine Learning Methods Applied to Ecoacoustics and Soundscape Monitoring",
        "type": "article",
        "url": "https://doi.org/10.1016/j.heliyon.2023.e20275",
        "volume": "9",
        "year": "2023"
    },
    "QuinnEtAl2022Soundscape": {
        "abstract": "Quinn et al. employ convolutional neural networks (CNNs) to classify ecoacoustic data and reveal temporal and geographic patterns in soundscape dynamics. By utilizing machine learning techniques, they demonstrate the effectiveness of CNNs in identifying and categorizing soundscape components. Their findings highlight the potential of this approach for understanding ecological indicators and monitoring biodiversity changes over time and space.",
        "author": "Colin A. Quinn, Leonardo Salas, Patrick Burns, Scott J. Goetz, Gurman Gill, Shrishail Baligar, Matthew L. Clark.",
        "doi": "10.1016/j.ecolind.2022.108831",
        "journal": "Ecological Indicators",
        "keywords": "soundscape classification, convolutional neural networks, ecoacoustic data, biodiversity monitoring, temporal patterns, geographic patterns",
        "title": "Soundscape Classification with Convolutional Neural Networks Reveals Temporal and Geographic Patterns in Ecoacoustic Data",
        "type": "article",
        "url": "https://doi.org/10.1016/j.ecolind.2022.108831",
        "volume": "138",
        "year": "2022"
    },
    "SethiEtAl2020Characterizing": {
        "abstract": "The study characterizes soundscapes across diverse ecosystems using a universal acoustic feature set. Researchers collected audio recordings from various ecosystems and utilized advanced acoustic feature extraction techniques to analyze the soundscape. Their results provide insights into the acoustic characteristics of different environments and highlight the potential of universal feature sets in understanding ecological dynamics. This interdisciplinary approach contributes to the broader understanding of ecosystem functioning and biodiversity monitoring.",
        "author": "Sarab S Sethi, Nick S Jones, Ben D Fulcher, Lorenzo Picinali, Dena Jane Clink, Holger Klinck, C David L Orme, Peter H Wrege, Robert M Ewers.",
        "doi": "10.1073/pnas.2004702117",
        "journal": "Proceedings of the National Academy of Sciences",
        "keywords": "soundscape ecology, acoustic feature extraction, biodiversity monitoring, ecosystem functioning",
        "number": "27",
        "title": "Characterizing Soundscapes Across Diverse Ecosystems Using a Universal Acoustic Feature Set",
        "type": "article",
        "url": "https://doi.org/10.1073/pnas.2004702117",
        "volume": "117",
        "year": "2020"
    },
    "Stowell2022Computational": {
        "abstract": "In this comprehensive review and roadmap, Stowell explores the intersection of computational bioacoustics and deep learning. He discusses the applications of deep learning techniques, particularly convolutional neural networks (CNNs), in analyzing bioacoustic data. The review covers various topics, including species classification, individual identification, and biodiversity monitoring using audio recordings. Stowell highlights the strengths and limitations of current approaches, as well as future directions and challenges in the field. This paper serves as a valuable resource for researchers and practitioners interested in leveraging deep learning for bioacoustic analysis.",
        "author": "Stowell D",
        "doi": "10.7717/peerj.13152",
        "journal": "PeerJ",
        "keywords": "computational bioacoustics, deep learning, convolutional neural networks, species classification, biodiversity monitoring",
        "title": "Computational Bioacoustics with Deep Learning: A Review and Roadmap",
        "type": "article",
        "url": "https://doi.org/10.7717%2Fpeerj.13152",
        "year": "2022"
    }
}});